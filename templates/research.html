<!doctype html>
<!-- paulirish.com/2008/conditional-stylesheets-vs-css-hacks-answer-neither/ -->
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
  <meta charset="utf-8">

  <!-- Page title -->
  <title>Projects</title>

  <meta name="description" content="">
  <meta name="author" content="">
  
  <!-- FontKit -->
<script src="//use.typekit.net/xrf3ije.js"></script>
<script>try{Typekit.load({ async: true });}catch(e){}</script>



  <!-- CSS -->
  <link rel="stylesheet" href="../css/style.css">

  <!-- Favicon -->
  <link rel="shortcut icon" href="../img/favicon.png">

  <!-- Scale -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Modernizr / Respond -->
  <script src="../js/libs/modernizr-2.0.6.min.js"></script>
</head>

<body class="pattern">
  <div class="wrapper">

  <!-- Header -->
  <header><div class="container">

    <h1 style="color:#c8cbceec;">Vashisht Madhavan</h1>

    <nav>
      <ul>
        <li class="selected">
           <li><a href="../index.html">home</a></li>
          <li><a href="research.html">research</a>
          <li><a href="about.html">about</a></li>
          <li><a href="../pdf/resume.pdf">resume</a>
      </ul>
    </nav>

    <div class="clearfix"></div>
  </div></header>
   
 <!-- Title -->
<div class="title" style="background-image:url(../img/equations.jpg);"><div class="container"><div class="full">
  <h2>research</h2>
</div></div></div>

<!-- Main content -->
  <section>
    <div class="full">
    
     <p>My research at <a href="https://www.uber.com/us/en/uberai/">UberAI</a> mainly focused on finding sample-efficient exploration methods for deep reinforcement learning. My previous work focused on domain adaptation and transfer learning in computer vision, specifically from virtual environments.</p>
   
    </div>
    <div class="clearfix"></div>
  </section>

  <!-- Scaling MAP-Elites -->
  <section id="scaling_map">
    <hr>
    <div class="two_thirds">
    <!-- BDD Data -->
    <h2>Scaling MAP-Elites to Deep Neuroevolution</h2>
    <p>Quality-Diversity (QD) algorithms, and MAP-Elites (ME) in particular, have proven very useful for a broad range of applications including enabling real robots to recover quickly from joint damage, solving strongly deceptive maze tasks or evolving robot morphologies to discover new gaits. However, present implementations of MAP-Elites and other QD algorithms seem to be limited to low-dimensional controllers with far fewer parameters than modern deep neural network models. In this paper, we propose to leverage the efficiency of Evolution Strategies (ES) to scale MAP-Elites to high-dimensional controllers parameterized by large neural networks. We design and evaluate a new hybrid algorithm called MAP-Elites with Evolution Strategies (ME-ES) for post-damage recovery in a difficult high-dimensional control task where traditional ME fails. Additionally,we show that ME-ES performs efficient exploration, on par with state-of-the-art exploration algorithms in high-dimensional control tasks with strongly deceptive rewards</p>

    <p><a href="https://arxiv.org/abs/2003.01825">Arxiv</a>  &nbsp; </p>
    </div>
    <div class="one_third">
        <img src="../img/scaling_map1.png" height=250 width=300 alt="">
    </div>
    <div class="clearfix"></div>
  </section>

  <!-- Atari Zoo -->
  <section id="zoo">
    <div class="one_third">
        <img src="../img/zoo_fig.png" height=300 width=300 alt="">
    </div>
    <div class="two_thirds">
      <h2>An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents</h2>
      <p>Much human and computational effort has aimed to improve how deep reinforcement learning algorithms perform on benchmarks such as the Atari Learning Environment. Comparatively less effort has focused on understanding what has been learned by such methods, and investigating and comparing the representations learned by different families of reinforcement learning (RL) algorithms. Sources of friction include the onerous computational requirements, and general logistical and architectural complications for running Deep RL algorithms at scale. We lessen this friction, by (1) training several algorithms at scale and releasing trained models, (2) integrating with a previous Deep RL model release, and (3) releasing code that makes it easy for anyone to load, visualize, and analyze such models. This paper introduces the Atari Zoo framework, which contains models trained across benchmark Atari games, in an easy-to-use format, as well as code that implements common modes of analysis and connects such models to a popular neural network visualization library. Further, to demonstrate the potential of this dataset and software package, we show initial quantitative and qualitative comparisons between the performance and representations of several deep RL algorithms, highlighting interesting and previously unknown distinctions between them.</p>

      <p><a href="https://arxiv.org/abs/1812.07069">arXiv</a>  &nbsp; <a href="https://eng.uber.com/atari-zoo-deep-reinforcement-learning/">Blog Post</a> &nbsp; <a href="https://github.com/uber-research/atari-model-zoo">Code</a></p>
    </div>
    <div class="clearfix"></div>
  </section>
   
  <!-- BDD 100K -->
  <section id="bdd_data">
    <div class="two_thirds">
	  <!-- BDD Data -->
	  <h2>BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling</h2>
	  <p>Datasets drive vision progress and autonomous driving is a critical vision application, yet existing driving datasets are impoverished in terms of visual content. Driving imagery is becoming plentiful, but annotation is slow and expensive, as annotation tools have not kept pace with the flood of data. Our first contribution is the design and implementation of a scalable annotation system that can provide a comprehensive set of image labels for large-scale driving datasets. Our second contribution is a new driving dataset, facilitated by our tooling, which is an order of magnitude larger than previous efforts, and is comprised of over 100K videos with diverse kinds of annotations including image level tagging, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models so that they are less likely to be surprised by new conditions.</p>

    <p><a href="https://arxiv.org/abs/1805.04687">Arxiv</a>  &nbsp; <a href="http://bdd-data.berkeley.edu/">Website</a> &nbsp;</p>
    </div>
    <div class="one_third">
        <img src="../img/bdd_100k.png" height=250 width=300 alt="">
    </div>
    <div class="clearfix"></div>
  </section>
   
  <!-- NSRA -->
  <section id="evolution">
    <div class="two_thirds">
    <h2>Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents</h2>
    <p>Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is not known how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and a version of QD we call NSR-ES, avoid local optima encountered by ES to achieve higher performance on tasks ranging from playing Atari to simulated robots learning to walk around a deceptive trap.</p>

    <p><a href="https://arxiv.org/abs/1712.06560">arXiv</a>  &nbsp; <a href="https://eng.uber.com/deep-neuroevolution/">Blog Post</a> &nbsp;</p>
    </div>
    <div class="one_third">
        <img src="../img/muj_wall.png" height=250 width=300 alt="">
    </div>

    <div class="clearfix"></div>
  </section>

  <!-- Deep GA -->
  <section id="evolution2">
    <div class="one_third">
        <img src="../img/frostbite.png" height=300 width=250 alt="">
    </div>
    <div class="two_thirds">
    <h2>Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning</h2>
    <p>Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm.</p>

    <p><a href="https://arxiv.org/abs/1712.06567">arXiv</a>  &nbsp; <a href="https://eng.uber.com/deep-neuroevolution/">Blog Post</a> &nbsp;</p>
    </div>

    <div class="clearfix"></div>
  </section>

  <!-- Semi-Supervised Transfer -->
  <section id="bdd">   
    <div class="two_thirds">
    <!-- BDD -->
    <h2>Examining the Effects of Supervision for Transfer from Synthetic to Real Driving Domains</h2>
    <p>With the surge of autonomous driving efforts in industry, semantic understanding of road scenes has become more commercially relevant than ever. Semantic segmentation, or dense pixel prediction, has become a popular approach for scene understanding, as the meteoric rise of deep convolutional networks (CNNs) has led to significant progress in recent years. However, these deep networks require large amounts of data, labeled at the pixel level, which can be quite cumbersome to collect for the driving domain. As a result, many have looked to realistic virtual world simulators (i.e. video games), for collecting large volumes of labeled data. Despite the visual similarity between real and synthetic domains, models trained on synthetic street scenes show poor results when evaluated on real-world data. To compensate for this visual shift, supervision from the real domain is necessary. In this work, I examine how much real-world supervision is appropriate for effective transfer of models pretrained on synthetic. Utilizing recent methods of supervised and semi-supervised transfer for fully convolutional networks (FCNs), I achieve promising results with a very small amount of labeled data(~50 images). By also quantitatively measuring different levels of domain shift, I reveal how simple metrics about a synthetic domain can be used to infer the ability of network features to transfer.</p>

    <p><a href="../pdf/drive_transfer.pdf">PDF</a>  &nbsp; <a href="../pdf/drive_slides.pdf">Presentation</a> &nbsp;</p>
    </div>
    <div class="one_third">
        <img src="../img/net2.png" height=300 width=300 alt="">
    </div>

    <div class="clearfix"></div>
  </section>


  <!-- Image Net Delta -->
  <section id="finetune">
    <div class="one_third">
      <img src="../img/transfer_plot2.png" height=300 width=300 alt="">
    </div>
    <div class="two_thirds">
      <h2>Best Practices for Fine-Tuning Visual Classifiers to New Domains</h2>
      <p>Recent studies have shown that features from deep convolutional neural networks learned using large labeled datasets, like ImageNet, provide effective representations for a variety of visual recognition tasks. They achieve strong performance as generic features and are even more effective when fine-tuned to target datasets. However, details of the fine-tuning procedure across datasets and with different amount of labeled data are not well-studied and choosing the best fine-tuning method is often left to trial and error. In this work we systematically explore the design-space for fine-tuning and give recommendations based on two key characteristics of the target dataset: visual distance from source dataset and the amount of available training data. Through a comprehensive experimental analysis, we conclude, with a few exceptions, that it is best to copy as many layers of a pre-trained network as possible, and then adjust the level of fine-tuning based on the visual distance from source.</p>
      <p><a href="../pdf/practices-fine-tuning.pdf">PDF</a> &nbsp; <a href="../pdf/taskcv_poster.pdf">Presentation</a> &nbsp;</p>
    </div>
    <div class="clearfix"></div>
  </section>

  <!-- GAN -->
  <section id="gan">
    <div class="two_thirds">
      <h2>Image Generation from Captions Using Dual-Loss Generative Adversarial Networks</h2>
      <p> Deep Convolutional Generative Adversarial Networks (DCGANs) have become popular in recent months for their ability to effectively capture image distributions and generate realistic images. Recent work has also shown that conditional information provided to Generative Adversarial Networks (GANs) allows for deterministic control of generated images simply through the regulation of the conditional vector. Although many have hinted that language models could be used as conditional information for generating images from words, there are very few attempts using GANs, much less DCGANs. In this project we explore and analyze the results of image generation by encoding captions as conditional information to our DCCGAN. We use a subset of the MSCOCO dataset to evaluate our results. </p>
      <p><a href="../pdf/image-generation-captions.pdf">PDF</a> &nbsp;</p>
    </div>
    <div class="one_third">
      <img src="../img/network.png" height=300 width=300 alt="">
    </div>
    <div class="clearfix"></div>
  </section>
  <hr>

<div class="title" style="background-image:url(../img/equations.jpg);"><div class="container"><div class="full">
  <h2>PROJECTS</h2>
</div></div></div>
<section>
  <div class="full">
  
   <p>Aside from research, Ive had the chance to apply machine learning and statistical inference methods to a number of interesting domains</p>
 
  </div>
  <div class="clearfix"></div>
   </section>

<section>
      <div class="one_third">
  <img src="../img/nba_wins.png" height=400 width=325 alt="">
  </div>
    <div class="two_thirds">
      <br><br><br><br><br>
      <h2>Predicting NBA Games with Hidden Markov Models</h2>
      <p>Here I used autoregressive HMMs to predict regular season win/loss totals for each NBA team. These predictions factored in per-game statistics, opponent strength, and team talent level. The image on the left shows my predictions for the 2015-2016 NBA season.</p>
      <br><p><a href="https://github.com/VashishtMadhavan/HmmNba">GitHub</a> &nbsp; <a href="../pdf/hmm_nba.pdf">PDF</a></p>
  </div>
  <div class="clearfix"></div>
   </section>

     
   
     <section>
      <div class="two_thirds">
         <h2>DailyCal Rank</h2>
    <p> For this project, I wrote TextRank/PageRank web crawlers that extract key phrases and popular words from the website of the DailyCal, Berkeley's primary student news publication.</p>
    <br><p><a href="https://github.com/VashishtMadhavan/126MiniProj">GitHub</a> &nbsp; </p>
      </div>
      <div class="one_third">
        <img src="../img/dailycal.png" height=200 width=200 alt="">
  </div>
  <div class="clearfix"></div>
   </section>

<section>
      <div class="two_thirds">
  
 <iframe width="560" height="315" src="https://www.youtube.com/embed/tknFQULAAs0" frameborder="0" allowfullscreen></iframe>
  </div>
    <div class="one_third">
      <h2>Colorization of Grayscale Images with SVMs</h2>
      <p>This project uses data-driven optimization methods to color grayscale images. Check out the video for more details!</p>
      <br><p><a href="https://github.com/VashishtMadhavan/194FinalProject">GitHub</a> &nbsp; <a href="../pdf/color_report.pdf">PDF</a></p>
  </div>
  <div class="clearfix"></div>
   </section>

 
  <!-- Footer -->
  <footer>

    <div class="container masonry">


    </div>

      <div class="clearfix"></div>

    <div class="bottom"><div class="container">
      
      

       <nav class="one_third">
        <ul style="float: left">
         <li><a href="https://www.linkedin.com/in/vashishtmadhavan"><img src="../img/linkedin.png" height=30 width=30 alt=""></a></li>
          <li><a href="mailto:vashisht.madhavan@gmail.com"><img src="../img/email.png" height=30 width=30 alt=""></a></li>
          <li><a href="https://github.com/VashishtMadhavan"><img src="../img/github.png" height=40 width=40 alt=""></a></li>
        </ul>
      </nav>

      <nav class="two_thirds">
        <ul>
          <li><a href="../index.html">HOME</a></li>
          <li><a href="research.html">RESEARCH</a></li>
          <li><a href="about.html">ABOUT</a></li>
          <li><a href="../pdf/resume.pdf">RESUME</a></li>
        </ul>
      </nav>
      <div class="clearfix"></div>

    </div></div>

  </footer>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.6.2/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../js/libs/jquery-1.6.2.min.js"><\/script>')</script>

  <!-- Scripts -->
  <script defer src="../js/jquery.easing.1.3.js"></script>
  <script defer src="../js/jquery.fancybox-1.3.4.pack.js"></script>
  <script defer src="../js/jquery.masonry.min.js"></script>
  <script defer src="../js/jquery.roundabout.min.js"></script>
  <script defer src="../js/pretty.js"></script>
  <script defer src="../js/script.js"></script>
	
  
  </div>
  <div class="clearfix"></div>
</body>
</html>
